{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":48666,"databundleVersionId":5149531,"sourceType":"competition"},{"sourceId":5366079,"sourceType":"datasetVersion","datasetId":3114243},{"sourceId":5366227,"sourceType":"datasetVersion","datasetId":3114285},{"sourceId":5366451,"sourceType":"datasetVersion","datasetId":3114361},{"sourceId":7050478,"sourceType":"datasetVersion","datasetId":4057527}],"dockerImageVersionId":30461,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport torch\nimport random\n\ndef set_seed(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\nset_seed(520)","metadata":{"execution":{"iopub.status.busy":"2025-07-20T17:22:37.820320Z","iopub.execute_input":"2025-07-20T17:22:37.820811Z","iopub.status.idle":"2025-07-20T17:22:37.826880Z","shell.execute_reply.started":"2025-07-20T17:22:37.820776Z","shell.execute_reply":"2025-07-20T17:22:37.825705Z"},"trusted":true},"outputs":[],"execution_count":24},{"cell_type":"code","source":"import os \nimport json\nimport torch\nimport random\nfrom pathlib import Path\nfrom torch.utils.data import Dataset\nfrom torch.nn.utils.rnn import pad_sequence\n\nclass myDataset(Dataset):\n    def __init__(self, data_dir, segment_len=128):\n        self.data_dir = data_dir\n        self.segment_len = segment_len\n        \n        mapping_path = Path(data_dir) / 'mapping.json'\n        mapping = json.load(mapping_path.open())\n        self.speaker2id = mapping['speaker2id']\n        \n        metadata_path = Path(data_dir) / 'metadata.json'\n        metadata = json.load(open(metadata_path))['speakers']\n        \n        self.speaker_num = len(metadata.keys())\n        self.data = []\n        for speaker in metadata.keys():\n            for utterances in metadata[speaker]:\n                self.data.append([utterances['feature_path'], self.speaker2id[speaker]])\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, index):\n        feat_path, speaker = self.data[index]\n        mel = torch.load(os.path.join(self.data_dir, feat_path))\n        \n        if len(mel) > self.segment_len:\n            # 只取128长度的语音序列来预测speaker\n            start = random.randint(0, len(mel) - self.segment_len)\n            mel = torch.FloatTensor(mel[start:start+self.segment_len])\n        else:\n            # 这里面可能存在比128段的序列，但是在组成batch时会填充\n            mel = torch.FloatTensor(mel)  \n        \n        speaker = torch.FloatTensor([speaker]).long()\n        return mel, speaker\n    \n    def get_speaker_number(self):\n        return self.speaker_num\n    \n        ","metadata":{"execution":{"iopub.status.busy":"2025-07-20T17:22:39.894008Z","iopub.execute_input":"2025-07-20T17:22:39.894916Z","iopub.status.idle":"2025-07-20T17:22:39.905223Z","shell.execute_reply.started":"2025-07-20T17:22:39.894878Z","shell.execute_reply":"2025-07-20T17:22:39.904294Z"},"trusted":true},"outputs":[],"execution_count":25},{"cell_type":"code","source":"import json\nimport torch\nfrom pathlib import Path\n\n\nmapping_path = Path('/kaggle/input/ml2023springhw4/Dataset') / 'mapping.json'\nmapping = json.load(mapping_path.open())\nmapping  # dict类型\n","metadata":{"execution":{"iopub.status.busy":"2025-07-20T17:22:42.982061Z","iopub.execute_input":"2025-07-20T17:22:42.983069Z","iopub.status.idle":"2025-07-20T17:22:43.016360Z","shell.execute_reply.started":"2025-07-20T17:22:42.983028Z","shell.execute_reply":"2025-07-20T17:22:43.015475Z"},"trusted":true},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"{'speaker2id': {'id00464': 0,\n  'id00559': 1,\n  'id00578': 2,\n  'id00905': 3,\n  'id01920': 4,\n  'id02368': 5,\n  'id04048': 6,\n  'id04070': 7,\n  'id04239': 8,\n  'id05743': 9,\n  'id06674': 10,\n  'id07109': 11,\n  'id00393': 12,\n  'id00805': 13,\n  'id01447': 14,\n  'id02546': 15,\n  'id03946': 16,\n  'id04282': 17,\n  'id04503': 18,\n  'id05574': 19,\n  'id06732': 20,\n  'id06922': 21,\n  'id07191': 22,\n  'id07198': 23,\n  'id07448': 24,\n  'id07664': 25,\n  'id07717': 26,\n  'id08185': 27,\n  'id08483': 28,\n  'id08806': 29,\n  'id09154': 30,\n  'id00931': 31,\n  'id01371': 32,\n  'id01488': 33,\n  'id01534': 34,\n  'id02011': 35,\n  'id02051': 36,\n  'id02104': 37,\n  'id02562': 38,\n  'id03816': 39,\n  'id03853': 40,\n  'id03858': 41,\n  'id04483': 42,\n  'id04802': 43,\n  'id04946': 44,\n  'id05508': 45,\n  'id05627': 46,\n  'id06422': 47,\n  'id09054': 48,\n  'id09094': 49,\n  'id09188': 50,\n  'id00036': 51,\n  'id00087': 52,\n  'id00388': 53,\n  'id00924': 54,\n  'id01109': 55,\n  'id01120': 56,\n  'id01503': 57,\n  'id02485': 58,\n  'id03017': 59,\n  'id03300': 60,\n  'id03873': 61,\n  'id04052': 62,\n  'id05194': 63,\n  'id05547': 64,\n  'id06595': 65,\n  'id06810': 66,\n  'id07697': 67,\n  'id08909': 68,\n  'id00515': 69,\n  'id00548': 70,\n  'id00934': 71,\n  'id01112': 72,\n  'id01230': 73,\n  'id01571': 74,\n  'id01578': 75,\n  'id03619': 76,\n  'id04565': 77,\n  'id04751': 78,\n  'id04835': 79,\n  'id06081': 80,\n  'id06106': 81,\n  'id06594': 82,\n  'id07055': 83,\n  'id08015': 84,\n  'id08094': 85,\n  'id08203': 86,\n  'id00251': 87,\n  'id00356': 88,\n  'id01026': 89,\n  'id01535': 90,\n  'id02018': 91,\n  'id02084': 92,\n  'id02185': 93,\n  'id02329': 94,\n  'id03279': 95,\n  'id04353': 96,\n  'id04567': 97,\n  'id04723': 98,\n  'id05162': 99,\n  'id05197': 100,\n  'id05483': 101,\n  'id05522': 102,\n  'id05568': 103,\n  'id05607': 104,\n  'id06134': 105,\n  'id06231': 106,\n  'id06301': 107,\n  'id06332': 108,\n  'id07123': 109,\n  'id07917': 110,\n  'id07989': 111,\n  'id09125': 112,\n  'id09263': 113,\n  'id00475': 114,\n  'id01624': 115,\n  'id02303': 116,\n  'id02355': 117,\n  'id02731': 118,\n  'id03009': 119,\n  'id03221': 120,\n  'id04227': 121,\n  'id05032': 122,\n  'id06019': 123,\n  'id06172': 124,\n  'id06890': 125,\n  'id06920': 126,\n  'id07009': 127,\n  'id07683': 128,\n  'id08281': 129,\n  'id01530': 130,\n  'id01657': 131,\n  'id01785': 132,\n  'id01878': 133,\n  'id02570': 134,\n  'id03400': 135,\n  'id03449': 136,\n  'id03860': 137,\n  'id04243': 138,\n  'id04609': 139,\n  'id04877': 140,\n  'id04898': 141,\n  'id04981': 142,\n  'id05346': 143,\n  'id05510': 144,\n  'id05905': 145,\n  'id07454': 146,\n  'id07682': 147,\n  'id07706': 148,\n  'id07876': 149,\n  'id08179': 150,\n  'id08782': 151,\n  'id09112': 152,\n  'id09119': 153,\n  'id00488': 154,\n  'id00571': 155,\n  'id00995': 156,\n  'id01237': 157,\n  'id01245': 158,\n  'id01246': 159,\n  'id01359': 160,\n  'id02152': 161,\n  'id04958': 162,\n  'id05487': 163,\n  'id05940': 164,\n  'id06255': 165,\n  'id06380': 166,\n  'id06397': 167,\n  'id06860': 168,\n  'id07960': 169,\n  'id07991': 170,\n  'id08315': 171,\n  'id08840': 172,\n  'id09160': 173,\n  'id09262': 174,\n  'id00167': 175,\n  'id00766': 176,\n  'id00867': 177,\n  'id01084': 178,\n  'id01185': 179,\n  'id01780': 180,\n  'id02114': 181,\n  'id02228': 182,\n  'id02473': 183,\n  'id02563': 184,\n  'id02607': 185,\n  'id02795': 186,\n  'id02882': 187,\n  'id02963': 188,\n  'id03318': 189,\n  'id03746': 190,\n  'id04301': 191,\n  'id04528': 192,\n  'id05373': 193,\n  'id05392': 194,\n  'id06011': 195,\n  'id06406': 196,\n  'id06486': 197,\n  'id07049': 198,\n  'id07254': 199,\n  'id07496': 200,\n  'id07581': 201,\n  'id07783': 202,\n  'id07977': 203,\n  'id08567': 204,\n  'id08597': 205,\n  'id09224': 206,\n  'id00129': 207,\n  'id01728': 208,\n  'id01830': 209,\n  'id01874': 210,\n  'id02359': 211,\n  'id02808': 212,\n  'id02874': 213,\n  'id03353': 214,\n  'id03785': 215,\n  'id04166': 216,\n  'id04292': 217,\n  'id06110': 218,\n  'id06392': 219,\n  'id06651': 220,\n  'id07103': 221,\n  'id08175': 222,\n  'id08365': 223,\n  'id08588': 224,\n  'id09175': 225,\n  'id00391': 226,\n  'id01609': 227,\n  'id03530': 228,\n  'id03569': 229,\n  'id03890': 230,\n  'id03936': 231,\n  'id05668': 232,\n  'id05966': 233,\n  'id06438': 234,\n  'id07493': 235,\n  'id08440': 236,\n  'id08570': 237,\n  'id08628': 238,\n  'id08870': 239,\n  'id00058': 240,\n  'id00407': 241,\n  'id00669': 242,\n  'id00701': 243,\n  'id00851': 244,\n  'id01105': 245,\n  'id01122': 246,\n  'id01695': 247,\n  'id01699': 248,\n  'id01946': 249,\n  'id02210': 250,\n  'id03388': 251,\n  'id03729': 252,\n  'id03876': 253,\n  'id03887': 254,\n  'id04686': 255,\n  'id05623': 256,\n  'id06459': 257,\n  'id07051': 258,\n  'id07362': 259,\n  'id07678': 260,\n  'id07911': 261,\n  'id08126': 262,\n  'id08884': 263,\n  'id09216': 264,\n  'id09234': 265,\n  'id00059': 266,\n  'id00649': 267,\n  'id00740': 268,\n  'id00969': 269,\n  'id01031': 270,\n  'id01199': 271,\n  'id01500': 272,\n  'id01748': 273,\n  'id02134': 274,\n  'id02472': 275,\n  'id04781': 276,\n  'id05895': 277,\n  'id05963': 278,\n  'id06417': 279,\n  'id06940': 280,\n  'id07189': 281,\n  'id07199': 282,\n  'id08605': 283,\n  'id00099': 284,\n  'id00371': 285,\n  'id00402': 286,\n  'id01866': 287,\n  'id02234': 288,\n  'id02469': 289,\n  'id02838': 290,\n  'id02904': 291,\n  'id03196': 292,\n  'id03865': 293,\n  'id04028': 294,\n  'id04665': 295,\n  'id04745': 296,\n  'id04956': 297,\n  'id05558': 298,\n  'id07210': 299,\n  'id07972': 300,\n  'id08580': 301,\n  'id00714': 302,\n  'id01085': 303,\n  'id01234': 304,\n  'id01972': 305,\n  'id02432': 306,\n  'id02944': 307,\n  'id03245': 308,\n  'id03348': 309,\n  'id03448': 310,\n  'id03592': 311,\n  'id03703': 312,\n  'id04336': 313,\n  'id04379': 314,\n  'id06042': 315,\n  'id06311': 316,\n  'id08305': 317,\n  'id08564': 318,\n  'id08883': 319,\n  'id00039': 320,\n  'id00416': 321,\n  'id01104': 322,\n  'id02780': 323,\n  'id02936': 324,\n  'id03057': 325,\n  'id03518': 326,\n  'id03914': 327,\n  'id03953': 328,\n  'id04175': 329,\n  'id04436': 330,\n  'id04687': 331,\n  'id05211': 332,\n  'id05282': 333,\n  'id05468': 334,\n  'id05945': 335,\n  'id06350': 336,\n  'id06408': 337,\n  'id07333': 338,\n  'id07910': 339,\n  'id08221': 340,\n  'id08247': 341,\n  'id08471': 342,\n  'id08954': 343,\n  'id00275': 344,\n  'id00431': 345,\n  'id01826': 346,\n  'id02175': 347,\n  'id02892': 348,\n  'id03817': 349,\n  'id03955': 350,\n  'id04082': 351,\n  'id04309': 352,\n  'id04624': 353,\n  'id04897': 354,\n  'id05405': 355,\n  'id06554': 356,\n  'id07107': 357,\n  'id07170': 358,\n  'id07470': 359,\n  'id07561': 360,\n  'id08402': 361,\n  'id00557': 362,\n  'id00846': 363,\n  'id01270': 364,\n  'id01734': 365,\n  'id02212': 366,\n  'id03349': 367,\n  'id03689': 368,\n  'id03694': 369,\n  'id03715': 370,\n  'id03900': 371,\n  'id04531': 372,\n  'id04962': 373,\n  'id05470': 374,\n  'id05509': 375,\n  'id05996': 376,\n  'id06052': 377,\n  'id06128': 378,\n  'id06155': 379,\n  'id06848': 380,\n  'id07001': 381,\n  'id07004': 382,\n  'id07367': 383,\n  'id08186': 384,\n  'id08581': 385,\n  'id09047': 386,\n  'id09243': 387,\n  'id00330': 388,\n  'id00414': 389,\n  'id00560': 390,\n  'id00778': 391,\n  'id00982': 392,\n  'id01014': 393,\n  'id01258': 394,\n  'id01915': 395,\n  'id01976': 396,\n  'id02024': 397,\n  'id02566': 398,\n  'id03363': 399,\n  'id03744': 400,\n  'id03958': 401,\n  'id03970': 402,\n  'id03974': 403,\n  'id03985': 404,\n  'id04703': 405,\n  'id04733': 406,\n  'id05423': 407,\n  'id05441': 408,\n  'id05649': 409,\n  'id05881': 410,\n  'id06483': 411,\n  'id07745': 412,\n  'id00508': 413,\n  'id00841': 414,\n  'id01561': 415,\n  'id02164': 416,\n  'id03000': 417,\n  'id03188': 418,\n  'id03286': 419,\n  'id03782': 420,\n  'id03912': 421,\n  'id04938': 422,\n  'id05303': 423,\n  'id05870': 424,\n  'id07338': 425,\n  'id07361': 426,\n  'id08755': 427,\n  'id09039': 428,\n  'id00506': 429,\n  'id01238': 430,\n  'id01660': 431,\n  'id01885': 432,\n  'id02037': 433,\n  'id02426': 434,\n  'id02545': 435,\n  'id03074': 436,\n  'id03412': 437,\n  'id03710': 438,\n  'id03893': 439,\n  'id04425': 440,\n  'id04847': 441,\n  'id04940': 442,\n  'id05985': 443,\n  'id06044': 444,\n  'id06146': 445,\n  'id06304': 446,\n  'id06909': 447,\n  'id07492': 448,\n  'id07533': 449,\n  'id08329': 450,\n  'id08340': 451,\n  'id08757': 452,\n  'id08850': 453,\n  'id08971': 454,\n  'id00810': 455,\n  'id00964': 456,\n  'id01582': 457,\n  'id01629': 458,\n  'id01686': 459,\n  'id01841': 460,\n  'id02196': 461,\n  'id02448': 462,\n  'id02702': 463,\n  'id03211': 464,\n  'id03749': 465,\n  'id03938': 466,\n  'id04935': 467,\n  'id04973': 468,\n  'id05000': 469,\n  'id05023': 470,\n  'id05449': 471,\n  'id05779': 472,\n  'id06462': 473,\n  'id06893': 474,\n  'id07177': 475,\n  'id07453': 476,\n  'id00865': 477,\n  'id01241': 478,\n  'id01251': 479,\n  'id01993': 480,\n  'id02080': 481,\n  'id02288': 482,\n  'id02597': 483,\n  'id02886': 484,\n  'id03402': 485,\n  'id03618': 486,\n  'id04895': 487,\n  'id05018': 488,\n  'id05520': 489,\n  'id05535': 490,\n  'id05624': 491,\n  'id05660': 492,\n  'id05727': 493,\n  'id05762': 494,\n  'id06277': 495,\n  'id07285': 496,\n  'id08642': 497,\n  'id01036': 498,\n  'id01080': 499,\n  'id01449': 500,\n  'id01628': 501,\n  'id02338': 502,\n  'id02575': 503,\n  'id03373': 504,\n  'id03924': 505,\n  'id03977': 506,\n  'id04016': 507,\n  'id04196': 508,\n  'id04215': 509,\n  'id05564': 510,\n  'id06270': 511,\n  'id07218': 512,\n  'id07528': 513,\n  'id07738': 514,\n  'id01049': 515,\n  'id02583': 516,\n  'id04077': 517,\n  'id04527': 518,\n  'id04756': 519,\n  'id05044': 520,\n  'id05096': 521,\n  'id05687': 522,\n  'id06643': 523,\n  'id06709': 524,\n  'id07244': 525,\n  'id07343': 526,\n  'id07474': 527,\n  'id07511': 528,\n  'id08074': 529,\n  'id08382': 530,\n  'id08417': 531,\n  'id08511': 532,\n  'id08903': 533,\n  'id02097': 534,\n  'id02231': 535,\n  'id05921': 536,\n  'id06068': 537,\n  'id06097': 538,\n  'id06428': 539,\n  'id06609': 540,\n  'id08320': 541,\n  'id08627': 542,\n  'id09271': 543,\n  'id00297': 544,\n  'id00801': 545,\n  'id02038': 546,\n  'id03092': 547,\n  'id03136': 548,\n  'id05106': 549,\n  'id05388': 550,\n  'id06073': 551,\n  'id07368': 552,\n  'id07580': 553,\n  'id08595': 554,\n  'id00365': 555,\n  'id00387': 556,\n  'id00426': 557,\n  'id00526': 558,\n  'id00781': 559,\n  'id01471': 560,\n  'id03422': 561,\n  'id04246': 562,\n  'id04886': 563,\n  'id05264': 564,\n  'id05445': 565,\n  'id06233': 566,\n  'id06234': 567,\n  'id07643': 568,\n  'id08646': 569,\n  'id08740': 570,\n  'id00322': 571,\n  'id00739': 572,\n  'id01528': 573,\n  'id01637': 574,\n  'id01666': 575,\n  'id02475': 576,\n  'id03819': 577,\n  'id07053': 578,\n  'id07526': 579,\n  'id07987': 580,\n  'id08327': 581,\n  'id00818': 582,\n  'id01001': 583,\n  'id01451': 584,\n  'id01502': 585,\n  'id02099': 586,\n  'id02295': 587,\n  'id02911': 588,\n  'id03993': 589,\n  'id04159': 590,\n  'id05082': 591,\n  'id05263': 592,\n  'id05284': 593,\n  'id06050': 594,\n  'id06962': 595,\n  'id07395': 596,\n  'id07696': 597,\n  'id08020': 598,\n  'id00206': 599},\n 'id2speaker': {'0': 'id00464',\n  '1': 'id00559',\n  '2': 'id00578',\n  '3': 'id00905',\n  '4': 'id01920',\n  '5': 'id02368',\n  '6': 'id04048',\n  '7': 'id04070',\n  '8': 'id04239',\n  '9': 'id05743',\n  '10': 'id06674',\n  '11': 'id07109',\n  '12': 'id00393',\n  '13': 'id00805',\n  '14': 'id01447',\n  '15': 'id02546',\n  '16': 'id03946',\n  '17': 'id04282',\n  '18': 'id04503',\n  '19': 'id05574',\n  '20': 'id06732',\n  '21': 'id06922',\n  '22': 'id07191',\n  '23': 'id07198',\n  '24': 'id07448',\n  '25': 'id07664',\n  '26': 'id07717',\n  '27': 'id08185',\n  '28': 'id08483',\n  '29': 'id08806',\n  '30': 'id09154',\n  '31': 'id00931',\n  '32': 'id01371',\n  '33': 'id01488',\n  '34': 'id01534',\n  '35': 'id02011',\n  '36': 'id02051',\n  '37': 'id02104',\n  '38': 'id02562',\n  '39': 'id03816',\n  '40': 'id03853',\n  '41': 'id03858',\n  '42': 'id04483',\n  '43': 'id04802',\n  '44': 'id04946',\n  '45': 'id05508',\n  '46': 'id05627',\n  '47': 'id06422',\n  '48': 'id09054',\n  '49': 'id09094',\n  '50': 'id09188',\n  '51': 'id00036',\n  '52': 'id00087',\n  '53': 'id00388',\n  '54': 'id00924',\n  '55': 'id01109',\n  '56': 'id01120',\n  '57': 'id01503',\n  '58': 'id02485',\n  '59': 'id03017',\n  '60': 'id03300',\n  '61': 'id03873',\n  '62': 'id04052',\n  '63': 'id05194',\n  '64': 'id05547',\n  '65': 'id06595',\n  '66': 'id06810',\n  '67': 'id07697',\n  '68': 'id08909',\n  '69': 'id00515',\n  '70': 'id00548',\n  '71': 'id00934',\n  '72': 'id01112',\n  '73': 'id01230',\n  '74': 'id01571',\n  '75': 'id01578',\n  '76': 'id03619',\n  '77': 'id04565',\n  '78': 'id04751',\n  '79': 'id04835',\n  '80': 'id06081',\n  '81': 'id06106',\n  '82': 'id06594',\n  '83': 'id07055',\n  '84': 'id08015',\n  '85': 'id08094',\n  '86': 'id08203',\n  '87': 'id00251',\n  '88': 'id00356',\n  '89': 'id01026',\n  '90': 'id01535',\n  '91': 'id02018',\n  '92': 'id02084',\n  '93': 'id02185',\n  '94': 'id02329',\n  '95': 'id03279',\n  '96': 'id04353',\n  '97': 'id04567',\n  '98': 'id04723',\n  '99': 'id05162',\n  '100': 'id05197',\n  '101': 'id05483',\n  '102': 'id05522',\n  '103': 'id05568',\n  '104': 'id05607',\n  '105': 'id06134',\n  '106': 'id06231',\n  '107': 'id06301',\n  '108': 'id06332',\n  '109': 'id07123',\n  '110': 'id07917',\n  '111': 'id07989',\n  '112': 'id09125',\n  '113': 'id09263',\n  '114': 'id00475',\n  '115': 'id01624',\n  '116': 'id02303',\n  '117': 'id02355',\n  '118': 'id02731',\n  '119': 'id03009',\n  '120': 'id03221',\n  '121': 'id04227',\n  '122': 'id05032',\n  '123': 'id06019',\n  '124': 'id06172',\n  '125': 'id06890',\n  '126': 'id06920',\n  '127': 'id07009',\n  '128': 'id07683',\n  '129': 'id08281',\n  '130': 'id01530',\n  '131': 'id01657',\n  '132': 'id01785',\n  '133': 'id01878',\n  '134': 'id02570',\n  '135': 'id03400',\n  '136': 'id03449',\n  '137': 'id03860',\n  '138': 'id04243',\n  '139': 'id04609',\n  '140': 'id04877',\n  '141': 'id04898',\n  '142': 'id04981',\n  '143': 'id05346',\n  '144': 'id05510',\n  '145': 'id05905',\n  '146': 'id07454',\n  '147': 'id07682',\n  '148': 'id07706',\n  '149': 'id07876',\n  '150': 'id08179',\n  '151': 'id08782',\n  '152': 'id09112',\n  '153': 'id09119',\n  '154': 'id00488',\n  '155': 'id00571',\n  '156': 'id00995',\n  '157': 'id01237',\n  '158': 'id01245',\n  '159': 'id01246',\n  '160': 'id01359',\n  '161': 'id02152',\n  '162': 'id04958',\n  '163': 'id05487',\n  '164': 'id05940',\n  '165': 'id06255',\n  '166': 'id06380',\n  '167': 'id06397',\n  '168': 'id06860',\n  '169': 'id07960',\n  '170': 'id07991',\n  '171': 'id08315',\n  '172': 'id08840',\n  '173': 'id09160',\n  '174': 'id09262',\n  '175': 'id00167',\n  '176': 'id00766',\n  '177': 'id00867',\n  '178': 'id01084',\n  '179': 'id01185',\n  '180': 'id01780',\n  '181': 'id02114',\n  '182': 'id02228',\n  '183': 'id02473',\n  '184': 'id02563',\n  '185': 'id02607',\n  '186': 'id02795',\n  '187': 'id02882',\n  '188': 'id02963',\n  '189': 'id03318',\n  '190': 'id03746',\n  '191': 'id04301',\n  '192': 'id04528',\n  '193': 'id05373',\n  '194': 'id05392',\n  '195': 'id06011',\n  '196': 'id06406',\n  '197': 'id06486',\n  '198': 'id07049',\n  '199': 'id07254',\n  '200': 'id07496',\n  '201': 'id07581',\n  '202': 'id07783',\n  '203': 'id07977',\n  '204': 'id08567',\n  '205': 'id08597',\n  '206': 'id09224',\n  '207': 'id00129',\n  '208': 'id01728',\n  '209': 'id01830',\n  '210': 'id01874',\n  '211': 'id02359',\n  '212': 'id02808',\n  '213': 'id02874',\n  '214': 'id03353',\n  '215': 'id03785',\n  '216': 'id04166',\n  '217': 'id04292',\n  '218': 'id06110',\n  '219': 'id06392',\n  '220': 'id06651',\n  '221': 'id07103',\n  '222': 'id08175',\n  '223': 'id08365',\n  '224': 'id08588',\n  '225': 'id09175',\n  '226': 'id00391',\n  '227': 'id01609',\n  '228': 'id03530',\n  '229': 'id03569',\n  '230': 'id03890',\n  '231': 'id03936',\n  '232': 'id05668',\n  '233': 'id05966',\n  '234': 'id06438',\n  '235': 'id07493',\n  '236': 'id08440',\n  '237': 'id08570',\n  '238': 'id08628',\n  '239': 'id08870',\n  '240': 'id00058',\n  '241': 'id00407',\n  '242': 'id00669',\n  '243': 'id00701',\n  '244': 'id00851',\n  '245': 'id01105',\n  '246': 'id01122',\n  '247': 'id01695',\n  '248': 'id01699',\n  '249': 'id01946',\n  '250': 'id02210',\n  '251': 'id03388',\n  '252': 'id03729',\n  '253': 'id03876',\n  '254': 'id03887',\n  '255': 'id04686',\n  '256': 'id05623',\n  '257': 'id06459',\n  '258': 'id07051',\n  '259': 'id07362',\n  '260': 'id07678',\n  '261': 'id07911',\n  '262': 'id08126',\n  '263': 'id08884',\n  '264': 'id09216',\n  '265': 'id09234',\n  '266': 'id00059',\n  '267': 'id00649',\n  '268': 'id00740',\n  '269': 'id00969',\n  '270': 'id01031',\n  '271': 'id01199',\n  '272': 'id01500',\n  '273': 'id01748',\n  '274': 'id02134',\n  '275': 'id02472',\n  '276': 'id04781',\n  '277': 'id05895',\n  '278': 'id05963',\n  '279': 'id06417',\n  '280': 'id06940',\n  '281': 'id07189',\n  '282': 'id07199',\n  '283': 'id08605',\n  '284': 'id00099',\n  '285': 'id00371',\n  '286': 'id00402',\n  '287': 'id01866',\n  '288': 'id02234',\n  '289': 'id02469',\n  '290': 'id02838',\n  '291': 'id02904',\n  '292': 'id03196',\n  '293': 'id03865',\n  '294': 'id04028',\n  '295': 'id04665',\n  '296': 'id04745',\n  '297': 'id04956',\n  '298': 'id05558',\n  '299': 'id07210',\n  '300': 'id07972',\n  '301': 'id08580',\n  '302': 'id00714',\n  '303': 'id01085',\n  '304': 'id01234',\n  '305': 'id01972',\n  '306': 'id02432',\n  '307': 'id02944',\n  '308': 'id03245',\n  '309': 'id03348',\n  '310': 'id03448',\n  '311': 'id03592',\n  '312': 'id03703',\n  '313': 'id04336',\n  '314': 'id04379',\n  '315': 'id06042',\n  '316': 'id06311',\n  '317': 'id08305',\n  '318': 'id08564',\n  '319': 'id08883',\n  '320': 'id00039',\n  '321': 'id00416',\n  '322': 'id01104',\n  '323': 'id02780',\n  '324': 'id02936',\n  '325': 'id03057',\n  '326': 'id03518',\n  '327': 'id03914',\n  '328': 'id03953',\n  '329': 'id04175',\n  '330': 'id04436',\n  '331': 'id04687',\n  '332': 'id05211',\n  '333': 'id05282',\n  '334': 'id05468',\n  '335': 'id05945',\n  '336': 'id06350',\n  '337': 'id06408',\n  '338': 'id07333',\n  '339': 'id07910',\n  '340': 'id08221',\n  '341': 'id08247',\n  '342': 'id08471',\n  '343': 'id08954',\n  '344': 'id00275',\n  '345': 'id00431',\n  '346': 'id01826',\n  '347': 'id02175',\n  '348': 'id02892',\n  '349': 'id03817',\n  '350': 'id03955',\n  '351': 'id04082',\n  '352': 'id04309',\n  '353': 'id04624',\n  '354': 'id04897',\n  '355': 'id05405',\n  '356': 'id06554',\n  '357': 'id07107',\n  '358': 'id07170',\n  '359': 'id07470',\n  '360': 'id07561',\n  '361': 'id08402',\n  '362': 'id00557',\n  '363': 'id00846',\n  '364': 'id01270',\n  '365': 'id01734',\n  '366': 'id02212',\n  '367': 'id03349',\n  '368': 'id03689',\n  '369': 'id03694',\n  '370': 'id03715',\n  '371': 'id03900',\n  '372': 'id04531',\n  '373': 'id04962',\n  '374': 'id05470',\n  '375': 'id05509',\n  '376': 'id05996',\n  '377': 'id06052',\n  '378': 'id06128',\n  '379': 'id06155',\n  '380': 'id06848',\n  '381': 'id07001',\n  '382': 'id07004',\n  '383': 'id07367',\n  '384': 'id08186',\n  '385': 'id08581',\n  '386': 'id09047',\n  '387': 'id09243',\n  '388': 'id00330',\n  '389': 'id00414',\n  '390': 'id00560',\n  '391': 'id00778',\n  '392': 'id00982',\n  '393': 'id01014',\n  '394': 'id01258',\n  '395': 'id01915',\n  '396': 'id01976',\n  '397': 'id02024',\n  '398': 'id02566',\n  '399': 'id03363',\n  '400': 'id03744',\n  '401': 'id03958',\n  '402': 'id03970',\n  '403': 'id03974',\n  '404': 'id03985',\n  '405': 'id04703',\n  '406': 'id04733',\n  '407': 'id05423',\n  '408': 'id05441',\n  '409': 'id05649',\n  '410': 'id05881',\n  '411': 'id06483',\n  '412': 'id07745',\n  '413': 'id00508',\n  '414': 'id00841',\n  '415': 'id01561',\n  '416': 'id02164',\n  '417': 'id03000',\n  '418': 'id03188',\n  '419': 'id03286',\n  '420': 'id03782',\n  '421': 'id03912',\n  '422': 'id04938',\n  '423': 'id05303',\n  '424': 'id05870',\n  '425': 'id07338',\n  '426': 'id07361',\n  '427': 'id08755',\n  '428': 'id09039',\n  '429': 'id00506',\n  '430': 'id01238',\n  '431': 'id01660',\n  '432': 'id01885',\n  '433': 'id02037',\n  '434': 'id02426',\n  '435': 'id02545',\n  '436': 'id03074',\n  '437': 'id03412',\n  '438': 'id03710',\n  '439': 'id03893',\n  '440': 'id04425',\n  '441': 'id04847',\n  '442': 'id04940',\n  '443': 'id05985',\n  '444': 'id06044',\n  '445': 'id06146',\n  '446': 'id06304',\n  '447': 'id06909',\n  '448': 'id07492',\n  '449': 'id07533',\n  '450': 'id08329',\n  '451': 'id08340',\n  '452': 'id08757',\n  '453': 'id08850',\n  '454': 'id08971',\n  '455': 'id00810',\n  '456': 'id00964',\n  '457': 'id01582',\n  '458': 'id01629',\n  '459': 'id01686',\n  '460': 'id01841',\n  '461': 'id02196',\n  '462': 'id02448',\n  '463': 'id02702',\n  '464': 'id03211',\n  '465': 'id03749',\n  '466': 'id03938',\n  '467': 'id04935',\n  '468': 'id04973',\n  '469': 'id05000',\n  '470': 'id05023',\n  '471': 'id05449',\n  '472': 'id05779',\n  '473': 'id06462',\n  '474': 'id06893',\n  '475': 'id07177',\n  '476': 'id07453',\n  '477': 'id00865',\n  '478': 'id01241',\n  '479': 'id01251',\n  '480': 'id01993',\n  '481': 'id02080',\n  '482': 'id02288',\n  '483': 'id02597',\n  '484': 'id02886',\n  '485': 'id03402',\n  '486': 'id03618',\n  '487': 'id04895',\n  '488': 'id05018',\n  '489': 'id05520',\n  '490': 'id05535',\n  '491': 'id05624',\n  '492': 'id05660',\n  '493': 'id05727',\n  '494': 'id05762',\n  '495': 'id06277',\n  '496': 'id07285',\n  '497': 'id08642',\n  '498': 'id01036',\n  '499': 'id01080',\n  '500': 'id01449',\n  '501': 'id01628',\n  '502': 'id02338',\n  '503': 'id02575',\n  '504': 'id03373',\n  '505': 'id03924',\n  '506': 'id03977',\n  '507': 'id04016',\n  '508': 'id04196',\n  '509': 'id04215',\n  '510': 'id05564',\n  '511': 'id06270',\n  '512': 'id07218',\n  '513': 'id07528',\n  '514': 'id07738',\n  '515': 'id01049',\n  '516': 'id02583',\n  '517': 'id04077',\n  '518': 'id04527',\n  '519': 'id04756',\n  '520': 'id05044',\n  '521': 'id05096',\n  '522': 'id05687',\n  '523': 'id06643',\n  '524': 'id06709',\n  '525': 'id07244',\n  '526': 'id07343',\n  '527': 'id07474',\n  '528': 'id07511',\n  '529': 'id08074',\n  '530': 'id08382',\n  '531': 'id08417',\n  '532': 'id08511',\n  '533': 'id08903',\n  '534': 'id02097',\n  '535': 'id02231',\n  '536': 'id05921',\n  '537': 'id06068',\n  '538': 'id06097',\n  '539': 'id06428',\n  '540': 'id06609',\n  '541': 'id08320',\n  '542': 'id08627',\n  '543': 'id09271',\n  '544': 'id00297',\n  '545': 'id00801',\n  '546': 'id02038',\n  '547': 'id03092',\n  '548': 'id03136',\n  '549': 'id05106',\n  '550': 'id05388',\n  '551': 'id06073',\n  '552': 'id07368',\n  '553': 'id07580',\n  '554': 'id08595',\n  '555': 'id00365',\n  '556': 'id00387',\n  '557': 'id00426',\n  '558': 'id00526',\n  '559': 'id00781',\n  '560': 'id01471',\n  '561': 'id03422',\n  '562': 'id04246',\n  '563': 'id04886',\n  '564': 'id05264',\n  '565': 'id05445',\n  '566': 'id06233',\n  '567': 'id06234',\n  '568': 'id07643',\n  '569': 'id08646',\n  '570': 'id08740',\n  '571': 'id00322',\n  '572': 'id00739',\n  '573': 'id01528',\n  '574': 'id01637',\n  '575': 'id01666',\n  '576': 'id02475',\n  '577': 'id03819',\n  '578': 'id07053',\n  '579': 'id07526',\n  '580': 'id07987',\n  '581': 'id08327',\n  '582': 'id00818',\n  '583': 'id01001',\n  '584': 'id01451',\n  '585': 'id01502',\n  '586': 'id02099',\n  '587': 'id02295',\n  '588': 'id02911',\n  '589': 'id03993',\n  '590': 'id04159',\n  '591': 'id05082',\n  '592': 'id05263',\n  '593': 'id05284',\n  '594': 'id06050',\n  '595': 'id06962',\n  '596': 'id07395',\n  '597': 'id07696',\n  '598': 'id08020',\n  '599': 'id00206'}}"},"metadata":{}},{"name":"stderr","text":"\nTrain:  49% 971/2000 [00:54<00:38, 26.53 step/s, accuracy=0.72, loss=9.73, step=34971]\u001b[A","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"metadata_path = Path('/kaggle/input/ml2023springhw4/Dataset') / 'metadata.json'\nmetadata = json.load(open(metadata_path))['speakers']\nmetadata.keys()","metadata":{"execution":{"iopub.status.busy":"2025-07-20T17:22:52.983817Z","iopub.execute_input":"2025-07-20T17:22:52.984546Z","iopub.status.idle":"2025-07-20T17:22:53.054773Z","shell.execute_reply.started":"2025-07-20T17:22:52.984514Z","shell.execute_reply":"2025-07-20T17:22:53.053852Z"},"trusted":true},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"dict_keys(['id03074', 'id05623', 'id06406', 'id01014', 'id02426', 'id01503', 'id05996', 'id05687', 'id03749', 'id01502', 'id04665', 'id05905', 'id07454', 'id02359', 'id00982', 'id05624', 'id02295', 'id00407', 'id09234', 'id06068', 'id01238', 'id07362', 'id04082', 'id02731', 'id01528', 'id05282', 'id09039', 'id07581', 'id07496', 'id02886', 'id02472', 'id04292', 'id03977', 'id08903', 'id03349', 'id06050', 'id03938', 'id07395', 'id04940', 'id01830', 'id03958', 'id05018', 'id03912', 'id00801', 'id04847', 'id00995', 'id03746', 'id03785', 'id07254', 'id04246', 'id00805', 'id05470', 'id01104', 'id08954', 'id09224', 'id07664', 'id02018', 'id01686', 'id09125', 'id07333', 'id08402', 'id06332', 'id04282', 'id04686', 'id09243', 'id07493', 'id02838', 'id03353', 'id01241', 'id07053', 'id01270', 'id05779', 'id05522', 'id08305', 'id07960', 'id06462', 'id04703', 'id07745', 'id01780', 'id00371', 'id05194', 'id04565', 'id09271', 'id04897', 'id09054', 'id01371', 'id03009', 'id03955', 'id00905', 'id04877', 'id05449', 'id00778', 'id03363', 'id05423', 'id06277', 'id01629', 'id07218', 'id08247', 'id02911', 'id08365', 'id02355', 'id04946', 'id08185', 'id02597', 'id06920', 'id02448', 'id05966', 'id01036', 'id08471', 'id08511', 'id03000', 'id01920', 'id08850', 'id02212', 'id03092', 'id01001', 'id00669', 'id02051', 'id01534', 'id04070', 'id04379', 'id03876', 'id07001', 'id00548', 'id08740', 'id01561', 'id04239', 'id03279', 'id04981', 'id00818', 'id03819', 'id02288', 'id03518', 'id05668', 'id08281', 'id00969', 'id02562', 'id07717', 'id01530', 'id07368', 'id02114', 'id03694', 'id00167', 'id08340', 'id02882', 'id05445', 'id07492', 'id02210', 'id00740', 'id02185', 'id01976', 'id04898', 'id01624', 'id01080', 'id01084', 'id03993', 'id05520', 'id03817', 'id03893', 'id03592', 'id03689', 'id00924', 'id02944', 'id02196', 'id07285', 'id03900', 'id07448', 'id02566', 'id01785', 'id00206', 'id00414', 'id06231', 'id01105', 'id06304', 'id05743', 'id09112', 'id01031', 'id02575', 'id06893', 'id05264', 'id01993', 'id07177', 'id06052', 'id00275', 'id05627', 'id05649', 'id00039', 'id02228', 'id02011', 'id03618', 'id00036', 'id05346', 'id08126', 'id00416', 'id04733', 'id05510', 'id01628', 'id05921', 'id02904', 'id03970', 'id06709', 'id00322', 'id00571', 'id00841', 'id00810', 'id05762', 'id07977', 'id01578', 'id06301', 'id02563', 'id03703', 'id01500', 'id06042', 'id01449', 'id07533', 'id03412', 'id04028', 'id07972', 'id04353', 'id02097', 'id00129', 'id07643', 'id04503', 'id07191', 'id09094', 'id07696', 'id05284', 'id06155', 'id08595', 'id04436', 'id06417', 'id06097', 'id06233', 'id07199', 'id00508', 'id03890', 'id05547', 'id06848', 'id05963', 'id03782', 'id05096', 'id08483', 'id05000', 'id03936', 'id01582', 'id03017', 'id02546', 'id04483', 'id01874', 'id03853', 'id02038', 'id04196', 'id00964', 'id09216', 'id02874', 'id06110', 'id07107', 'id02303', 'id06128', 'id03946', 'id01109', 'id00851', 'id05211', 'id06940', 'id07474', 'id03448', 'id08186', 'id08642', 'id02080', 'id08755', 'id07453', 'id01666', 'id07109', 'id09047', 'id00431', 'id08320', 'id06172', 'id03245', 'id00388', 'id08757', 'id06106', 'id04624', 'id06651', 'id01728', 'id05945', 'id00475', 'id02607', 'id05940', 'id04745', 'id00087', 'id01230', 'id01609', 'id00649', 'id08327', 'id04802', 'id02583', 'id06643', 'id00365', 'id07004', 'id06486', 'id07706', 'id03710', 'id03715', 'id08909', 'id00393', 'id00781', 'id06255', 'id06909', 'id02234', 'id05441', 'id01120', 'id06044', 'id01359', 'id01251', 'id08094', 'id00058', 'id01085', 'id02432', 'id01946', 'id05508', 'id01488', 'id01185', 'id06146', 'id04243', 'id08840', 'id06081', 'id07198', 'id03974', 'id05162', 'id07170', 'id02329', 'id07244', 'id07991', 'id04048', 'id01471', 'id07911', 'id03914', 'id08564', 'id08221', 'id05483', 'id05535', 'id04301', 'id00701', 'id06380', 'id07987', 'id02475', 'id04166', 'id07189', 'id02892', 'id08382', 'id02104', 'id07343', 'id07123', 'id06019', 'id06860', 'id02936', 'id06674', 'id02338', 'id08627', 'id03318', 'id07103', 'id08884', 'id09188', 'id09175', 'id01245', 'id03402', 'id05023', 'id03569', 'id00402', 'id07876', 'id04781', 'id05607', 'id04425', 'id04958', 'id06459', 'id04531', 'id07783', 'id00488', 'id05303', 'id08580', 'id07470', 'id08567', 'id00426', 'id06594', 'id08870', 'id02152', 'id02037', 'id05870', 'id03858', 'id09119', 'id01878', 'id04052', 'id01571', 'id04723', 'id08175', 'id03865', 'id05881', 'id01660', 'id08020', 'id03400', 'id04309', 'id01699', 'id03188', 'id06073', 'id03211', 'id02702', 'id05106', 'id00714', 'id01535', 'id00391', 'id08015', 'id05032', 'id07526', 'id03196', 'id04016', 'id03422', 'id06270', 'id07528', 'id01451', 'id06234', 'id01026', 'id03388', 'id05568', 'id06392', 'id03873', 'id01695', 'id01866', 'id02164', 'id06134', 'id03729', 'id03286', 'id00330', 'id03985', 'id05405', 'id08646', 'id04956', 'id07055', 'id05660', 'id00867', 'id01199', 'id00099', 'id01237', 'id05197', 'id08203', 'id01915', 'id02099', 'id06011', 'id04528', 'id06350', 'id03530', 'id06732', 'id03744', 'id00560', 'id01748', 'id04886', 'id07210', 'id07580', 'id07917', 'id06890', 'id02134', 'id06595', 'id02485', 'id08315', 'id02469', 'id02795', 'id01234', 'id00356', 'id08440', 'id00506', 'id00739', 'id01122', 'id04835', 'id08597', 'id07009', 'id04973', 'id03619', 'id08588', 'id03860', 'id05558', 'id01734', 'id07989', 'id01972', 'id08628', 'id06428', 'id02231', 'id08782', 'id06554', 'id04938', 'id00557', 'id04175', 'id00515', 'id03221', 'id01258', 'id01826', 'id03057', 'id00934', 'id06609', 'id00526', 'id04895', 'id02570', 'id00387', 'id02024', 'id00059', 'id08605', 'id09263', 'id02545', 'id05985', 'id04935', 'id05487', 'id02963', 'id04609', 'id00578', 'id01885', 'id06438', 'id09154', 'id01447', 'id06922', 'id07361', 'id07338', 'id01112', 'id03348', 'id06810', 'id06311', 'id00251', 'id03449', 'id07367', 'id08570', 'id07049', 'id06962', 'id04215', 'id00846', 'id01246', 'id08179', 'id07682', 'id07051', 'id04527', 'id03953', 'id07561', 'id04687', 'id01841', 'id03136', 'id04962', 'id03924', 'id07678', 'id04336', 'id07697', 'id07511', 'id03373', 'id05727', 'id08806', 'id05263', 'id04751', 'id05373', 'id09160', 'id06408', 'id02473', 'id02808', 'id00559', 'id03887', 'id05044', 'id04159', 'id01049', 'id00464', 'id08883', 'id07683', 'id07738', 'id05388', 'id04077', 'id05082', 'id05895', 'id06422', 'id00766', 'id05509', 'id02780', 'id02084', 'id00297', 'id05392', 'id07910', 'id06397', 'id08074', 'id08581', 'id00931', 'id03300', 'id06483', 'id05574', 'id09262', 'id01657', 'id00865', 'id02368', 'id03816', 'id01637', 'id08417', 'id05468', 'id04567', 'id05564', 'id08329', 'id04227', 'id08971', 'id04756', 'id02175'])"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"mel = torch.load('/kaggle/input/ml2023springhw4/Dataset/uttr-0002067f80214182ab863378bdcdd68a.pt')\nmel.shape","metadata":{"execution":{"iopub.status.busy":"2025-07-20T17:22:55.939742Z","iopub.execute_input":"2025-07-20T17:22:55.940726Z","iopub.status.idle":"2025-07-20T17:22:55.948662Z","shell.execute_reply.started":"2025-07-20T17:22:55.940690Z","shell.execute_reply":"2025-07-20T17:22:55.947411Z"},"trusted":true},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"torch.Size([660, 40])"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, random_split\nfrom torch.nn.utils.rnn import pad_sequence\n\ndef collate_batch(batch):\n    mel, speaker = zip(*batch)  # 解配对\n    mel = pad_sequence(mel, batch_first=True, padding_value=-20)\n    return mel, torch.FloatTensor(speaker).long()\n\ndef get_dataloader(data_dir, batch_size, n_workers):\n    dataset = myDataset(data_dir)\n    speaker_num = dataset.get_speaker_number()\n    trainlen = int(0.9 * len(dataset))\n    lengths = [trainlen, len(dataset) - trainlen]\n    trainset, validset = random_split(dataset, lengths)  # dataset也是可以操作的\n    \n    train_loader = DataLoader(trainset,\n                             batch_size=batch_size,\n                             shuffle=True,\n                             drop_last=True,\n                             num_workers=n_workers,\n                             pin_memory=True,\n                             collate_fn=collate_batch)\n    \n    valid_loader = DataLoader(validset,\n                             batch_size=batch_size,\n                             num_workers=n_workers,\n                             drop_last=True,\n                             pin_memory=True,\n                             collate_fn=collate_batch)\n    \n    return train_loader, valid_loader, speaker_num","metadata":{"execution":{"iopub.status.busy":"2025-07-20T17:22:57.203164Z","iopub.execute_input":"2025-07-20T17:22:57.204183Z","iopub.status.idle":"2025-07-20T17:22:57.211617Z","shell.execute_reply.started":"2025-07-20T17:22:57.204143Z","shell.execute_reply":"2025-07-20T17:22:57.210631Z"},"trusted":true},"outputs":[],"execution_count":29},{"cell_type":"code","source":"!pip install conformer\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n# from conformer import ConformerBlock\nfrom conformer import Conformer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T17:22:59.337456Z","iopub.execute_input":"2025-07-20T17:22:59.338392Z","iopub.status.idle":"2025-07-20T17:23:07.741869Z","shell.execute_reply.started":"2025-07-20T17:22:59.338361Z","shell.execute_reply":"2025-07-20T17:23:07.740881Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: conformer in /opt/conda/lib/python3.7/site-packages (0.3.2)\nRequirement already satisfied: einops>=0.6.1 in /opt/conda/lib/python3.7/site-packages (from conformer) (0.6.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from conformer) (1.13.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->conformer) (4.4.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"class SelfAttentionPooling(nn.Module):\n    def __init__(self, input_dim):\n        super(SelfAttentionPooling, self).__init__()\n        self.W = nn.Linear(input_dim, 1)\n        \n    def forward(self, batch_rep):\n        softmax = nn.functional.softmax\n        \n        att_w = softmax(self.W(batch_rep).squeeze(-1)).unsqueeze(-1)\n        utter_rep = torch.sum(batch_rep * att_w, dim=1)\n\n        return utter_rep","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T17:23:09.723039Z","iopub.execute_input":"2025-07-20T17:23:09.723778Z","iopub.status.idle":"2025-07-20T17:23:09.729406Z","shell.execute_reply.started":"2025-07-20T17:23:09.723742Z","shell.execute_reply":"2025-07-20T17:23:09.728384Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"class CrossEntropyLabelSmooth(nn.Module):\n\n    def __init__(self, num_classes=600, epsilon=0.1, use_gpu=True):\n        super(CrossEntropyLabelSmooth, self).__init__()\n        self.num_classes = num_classes\n        self.epsilon = epsilon\n        self.use_gpu = use_gpu\n        self.logsoftmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, inputs, targets, use_label_smoothing=True):\n        log_probs = self.logsoftmax(inputs)\n        targets = torch.zeros(log_probs.size()).scatter_(1, targets.unsqueeze(1).data.cpu(), 1)\n        if self.use_gpu: targets = targets.to(torch.device('cuda'))\n        if use_label_smoothing:\n            targets = (1 - self.epsilon) * targets + self.epsilon / self.num_classes\n        loss = (- targets * log_probs).mean(0).sum()\n        return loss\n    \n\nclass AMSoftmaxLoss(nn.Module): #requires classification layer for normalization \n    def __init__(self, m=0.35, s=30, d=256, num_classes=600, use_gpu=True , epsilon=0.1):\n        super(AMSoftmaxLoss, self).__init__()\n        self.m = m\n        self.s = s \n        self.num_classes = num_classes\n        self.CrossEntropy = CrossEntropyLabelSmooth(self.num_classes , use_gpu=use_gpu)\n\n    def forward(self, features, labels , classifier):\n        # x = torch.rand(32,2048)\n        # label = torch.tensor([0,0,0,0,1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4,5,5,5,5,6,6,6,6,7,7,7,7,])\n        features = nn.functional.normalize(features, p=2, dim=1) # normalize the features\n        with torch.no_grad():\n            classifier.weight.div_(torch.norm(classifier.weight, dim=1, keepdim=True))\n\n        cos_angle = classifier(features)\n        cos_angle = torch.clamp( cos_angle , min = -1 , max = 1 ) \n        b = features.size(0)\n        for i in range(b):\n            cos_angle[i][labels[i]] = cos_angle[i][labels[i]]  - self.m \n        weighted_cos_angle = self.s * cos_angle\n        log_probs = self.CrossEntropy(weighted_cos_angle , labels, use_label_smoothing=True)\n        return log_probs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T17:23:11.897613Z","iopub.execute_input":"2025-07-20T17:23:11.898490Z","iopub.status.idle":"2025-07-20T17:23:11.909382Z","shell.execute_reply.started":"2025-07-20T17:23:11.898456Z","shell.execute_reply":"2025-07-20T17:23:11.908420Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"\n\nclass Classifier(nn.Module):\n    def __init__(self, d_model=80, n_spks=600, dropout=0.2):\n        super().__init__()\n        # 预处理网络，改变输入维度\n        self.prenet = nn.Linear(40, d_model)\n        self.encoder = Conformer(\n                dim = d_model,\n                depth = 2,          # 12 blocks\n                dim_head = 64,\n                heads = 8,\n                ff_mult = 4,\n                conv_expansion_factor = 2,\n                conv_kernel_size = 31,\n                attn_dropout = dropout,\n                ff_dropout = dropout,\n                conv_dropout = dropout\n            )\n        self.sap = SelfAttentionPooling(d_model)\n\n        self.pred_layer = nn.Linear(d_model, n_spks)\n        \n    def forward(self, mels):\n        out = self.prenet(mels)\n        out = out.permute(1, 0, 2)\n        out = self.encoder(out)\n        out = out.transpose(0, 1)\n        out = self.sap(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2025-07-20T17:23:14.133125Z","iopub.execute_input":"2025-07-20T17:23:14.133866Z","iopub.status.idle":"2025-07-20T17:23:14.141071Z","shell.execute_reply.started":"2025-07-20T17:23:14.133830Z","shell.execute_reply":"2025-07-20T17:23:14.139998Z"},"trusted":true},"outputs":[],"execution_count":34},{"cell_type":"code","source":"import math\n\nimport torch\nfrom torch.optim import Optimizer\nfrom torch.optim.lr_scheduler import LambdaLR\n\n\ndef get_cosine_schedule_with_warmup(\n    optimizer: Optimizer,\n    num_warmup_steps: int,\n    num_training_steps: int,\n    num_cycles: float = 0.5,\n    last_epoch: int = -1,\n):\n    \"\"\"\n    Create a schedule with a learning rate that decreases following the values of the cosine function between the\n    initial lr set in the optimizer to 0, after a warmup period during which it increases linearly between 0 and the\n    initial lr set in the optimizer.\n\n    Args:\n        optimizer (:class:`~torch.optim.Optimizer`):\n        The optimizer for which to schedule the learning rate.\n        num_warmup_steps (:obj:`int`):\n        The number of steps for the warmup phase.\n        num_training_steps (:obj:`int`):\n        The total number of training steps.\n        num_cycles (:obj:`float`, `optional`, defaults to 0.5):\n        The number of waves in the cosine schedule (the defaults is to just decrease from the max value to 0\n        following a half-cosine).\n        last_epoch (:obj:`int`, `optional`, defaults to -1):\n        The index of the last epoch when resuming training.\n\n    Return:\n        :obj:`torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n    \"\"\"\n    def lr_lambda(current_step):\n        # Warmup\n        if current_step < num_warmup_steps:\n            return float(current_step) / float(max(1, num_warmup_steps))\n        # decadence\n        progress = float(current_step - num_warmup_steps) / float(\n            max(1, num_training_steps - num_warmup_steps)\n        )\n        return max(\n            0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))\n        )\n\n    return LambdaLR(optimizer, lr_lambda, last_epoch)","metadata":{"execution":{"iopub.status.busy":"2025-07-20T17:23:22.987221Z","iopub.execute_input":"2025-07-20T17:23:22.987960Z","iopub.status.idle":"2025-07-20T17:23:22.995225Z","shell.execute_reply.started":"2025-07-20T17:23:22.987925Z","shell.execute_reply":"2025-07-20T17:23:22.994121Z"},"trusted":true},"outputs":[],"execution_count":36},{"cell_type":"code","source":"import torch\n\n\ndef model_fn(batch, model, criterion, device):\n    \"\"\"Forward a batch through the model.\"\"\"\n\n    mels, labels = batch\n    mels = mels.to(device)\n    labels = labels.to(device)\n    \n    outs = model(mels)\n    \n    loss = criterion(outs, labels, model.pred_layer)\n\n    # Get the speaker id with highest probability.\n    outs = model.pred_layer(outs)\n    preds = outs.argmax(1)\n    # Compute accuracy.\n    accuracy = torch.mean((preds == labels).float())\n\n    # Printout model's parameters\n#     for name, param in model.named_parameters():\n#         if param.requires_grad:\n#             print(name)\n        \n    return loss, accuracy","metadata":{"execution":{"iopub.status.busy":"2025-07-20T17:23:25.602702Z","iopub.execute_input":"2025-07-20T17:23:25.603407Z","iopub.status.idle":"2025-07-20T17:23:25.609499Z","shell.execute_reply.started":"2025-07-20T17:23:25.603376Z","shell.execute_reply":"2025-07-20T17:23:25.608497Z"},"trusted":true},"outputs":[],"execution_count":37},{"cell_type":"code","source":"from tqdm import tqdm\nimport torch\n\n\ndef valid(dataloader, model, criterion, device): \n    \"\"\"Validate on validation set.\"\"\"\n\n    model.eval()\n    running_loss = 0.0\n    running_accuracy = 0.0\n    pbar = tqdm(total=len(dataloader.dataset), ncols=0, desc=\"Valid\", unit=\" uttr\")\n\n    for i, batch in enumerate(dataloader):\n        with torch.no_grad():\n            loss, accuracy = model_fn(batch, model, criterion, device)\n            running_loss += loss.item()\n            running_accuracy += accuracy.item()\n\n        pbar.update(dataloader.batch_size)\n        pbar.set_postfix(\n            loss=f\"{running_loss / (i+1):.2f}\",\n            accuracy=f\"{running_accuracy / (i+1):.2f}\",\n        )\n\n    pbar.close()\n    model.train()\n\n    return running_accuracy / len(dataloader)","metadata":{"execution":{"iopub.status.busy":"2025-07-20T17:23:28.051471Z","iopub.execute_input":"2025-07-20T17:23:28.052375Z","iopub.status.idle":"2025-07-20T17:23:28.059007Z","shell.execute_reply.started":"2025-07-20T17:23:28.052338Z","shell.execute_reply":"2025-07-20T17:23:28.057982Z"},"trusted":true},"outputs":[],"execution_count":38},{"cell_type":"code","source":"from tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader, random_split\n\n\ndef parse_args():\n    \"\"\"arguments\"\"\"\n    config = {\n        \"data_dir\": \"/kaggle/input/ml2023springhw4/Dataset\",\n        \"save_path\": \"model.ckpt\",\n        \"batch_size\": 32,\n        \"n_workers\": 8,\n        \"valid_steps\": 2000,\n        \"warmup_steps\": 1000,\n        \"save_steps\": 10000,\n        \"total_steps\": 70000,\n    }\n\n    return config\n\n\ndef main(\n    data_dir,\n    save_path,\n    batch_size,\n    n_workers,\n    valid_steps,\n    warmup_steps,\n    total_steps,\n    save_steps,\n):\n    \"\"\"Main function.\"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"[Info]: Use {device} now!\")\n\n    train_loader, valid_loader, speaker_num = get_dataloader(data_dir, batch_size, n_workers)\n    train_iterator = iter(train_loader)\n    print(f\"[Info]: Finish loading data!\",flush = True)\n\n    model = Classifier(n_spks=speaker_num).to(device)\n#     criterion = nn.CrossEntropyLoss()\n    criterion = AMSoftmaxLoss(m=0.4, s=30)\n    optimizer = AdamW(model.parameters(), lr=1e-3)\n    scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n    print(f\"[Info]: Finish creating model!\",flush = True)\n\n    best_accuracy = -1.0\n    best_state_dict = None\n\n    pbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n\n    for step in range(total_steps):\n        # Get data\n        try:\n            batch = next(train_iterator) # (32, 128, 40)\n        except StopIteration:\n            train_iterator = iter(train_loader)\n            batch = next(train_iterator)\n\n        loss, accuracy = model_fn(batch, model, criterion, device)\n        batch_loss = loss.item()\n        batch_accuracy = accuracy.item()\n\n        # Updata model\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n\n        # Log\n        pbar.update()\n        pbar.set_postfix(\n            loss=f\"{batch_loss:.2f}\",\n            accuracy=f\"{batch_accuracy:.2f}\",\n            step=step + 1,\n        )\n\n        # Do validation\n        if (step + 1) % valid_steps == 0:\n            pbar.close()\n\n            valid_accuracy = valid(valid_loader, model, criterion, device)\n\n            # keep the best model\n            if valid_accuracy > best_accuracy:\n                best_accuracy = valid_accuracy\n                best_state_dict = model.state_dict()\n\n            pbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n\n        # Save the best model so far.\n        if (step + 1) % save_steps == 0 and best_state_dict is not None:\n            torch.save(best_state_dict, save_path)\n            pbar.write(f\"Step {step + 1}, best model saved. (accuracy={best_accuracy:.4f})\")\n\n    pbar.close()\n\n\nif __name__ == \"__main__\":\n    main(**parse_args())","metadata":{"execution":{"iopub.status.busy":"2025-07-20T17:23:30.409399Z","iopub.execute_input":"2025-07-20T17:23:30.410292Z","iopub.status.idle":"2025-07-20T18:06:24.289784Z","shell.execute_reply.started":"2025-07-20T17:23:30.410255Z","shell.execute_reply":"2025-07-20T18:06:24.288467Z"},"trusted":true},"outputs":[{"name":"stdout","text":"[Info]: Use cuda now!\n","output_type":"stream"},{"name":"stderr","text":"Train:   0% 0/2000 [27:28<?, ? step/s]\n","output_type":"stream"},{"name":"stdout","text":"[Info]: Finish loading data!\n[Info]: Finish creating model!\n","output_type":"stream"},{"name":"stderr","text":"Train:   0% 0/2000 [00:00<?, ? step/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  if __name__ == \"__main__\":\nTrain: 100% 2000/2000 [01:10<00:00, 28.30 step/s, accuracy=0.00, loss=16.72, step=2000]\nValid: 100% 5664/5667 [00:03<00:00, 1829.72 uttr/s, accuracy=0.03, loss=16.25]\nTrain: 100% 2000/2000 [01:10<00:00, 28.29 step/s, accuracy=0.16, loss=14.60, step=4000]\nValid: 100% 5664/5667 [00:03<00:00, 1777.13 uttr/s, accuracy=0.16, loss=15.08]\nTrain: 100% 2000/2000 [01:10<00:00, 28.53 step/s, accuracy=0.47, loss=13.31, step=6000]\nValid: 100% 5664/5667 [00:03<00:00, 1804.96 uttr/s, accuracy=0.29, loss=14.26]\nTrain: 100% 2000/2000 [01:10<00:00, 28.41 step/s, accuracy=0.31, loss=13.36, step=8000]\nValid: 100% 5664/5667 [00:03<00:00, 1805.06 uttr/s, accuracy=0.40, loss=13.56]\nTrain: 100% 2000/2000 [01:10<00:00, 28.46 step/s, accuracy=0.41, loss=13.36, step=1e+4]\nValid: 100% 5664/5667 [00:03<00:00, 1839.84 uttr/s, accuracy=0.48, loss=12.97]\n                                      \nTrain:   0% 0/2000 [00:00<?, ? step/s]                                                \nTrain:   0% 5/2000 [00:00<01:27, 22.84 step/s, accuracy=0.50, loss=13.06, step=1e+4]1]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Step 10000, best model saved. (accuracy=0.4762)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [01:10<00:00, 28.22 step/s, accuracy=0.72, loss=10.83, step=12000]\nValid: 100% 5664/5667 [00:03<00:00, 1775.19 uttr/s, accuracy=0.54, loss=12.48]\nTrain: 100% 2000/2000 [01:10<00:00, 28.36 step/s, accuracy=0.75, loss=10.61, step=14000]\nValid: 100% 5664/5667 [00:03<00:00, 1824.63 uttr/s, accuracy=0.58, loss=12.02]\nTrain: 100% 2000/2000 [01:11<00:00, 28.13 step/s, accuracy=0.78, loss=10.68, step=16000]\nValid: 100% 5664/5667 [00:03<00:00, 1808.68 uttr/s, accuracy=0.60, loss=11.71]\nTrain: 100% 2000/2000 [01:10<00:00, 28.47 step/s, accuracy=0.72, loss=10.10, step=18000]\nValid: 100% 5664/5667 [00:03<00:00, 1815.65 uttr/s, accuracy=0.61, loss=11.59]\nTrain: 100% 2000/2000 [01:10<00:00, 28.37 step/s, accuracy=0.62, loss=12.10, step=2e+4] \nValid: 100% 5664/5667 [00:03<00:00, 1826.32 uttr/s, accuracy=0.64, loss=11.26]\n                                      \nTrain:   0% 0/2000 [00:00<?, ? step/s]                                                \nTrain:   0% 5/2000 [00:00<01:26, 23.12 step/s, accuracy=0.62, loss=10.64, step=2e+4]1]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Step 20000, best model saved. (accuracy=0.6393)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [01:10<00:00, 28.28 step/s, accuracy=0.84, loss=8.74, step=22000] \nValid: 100% 5664/5667 [00:03<00:00, 1795.94 uttr/s, accuracy=0.65, loss=11.05]\nTrain: 100% 2000/2000 [01:10<00:00, 28.28 step/s, accuracy=0.75, loss=10.63, step=24000]\nValid: 100% 5664/5667 [00:03<00:00, 1823.66 uttr/s, accuracy=0.67, loss=10.86]\nTrain: 100% 2000/2000 [01:10<00:00, 28.55 step/s, accuracy=0.69, loss=10.91, step=26000]\nValid: 100% 5664/5667 [00:03<00:00, 1803.21 uttr/s, accuracy=0.67, loss=10.79]\nTrain: 100% 2000/2000 [01:11<00:00, 28.14 step/s, accuracy=0.69, loss=11.10, step=28000]\nValid: 100% 5664/5667 [00:03<00:00, 1786.71 uttr/s, accuracy=0.68, loss=10.56]\nTrain: 100% 2000/2000 [01:10<00:00, 28.48 step/s, accuracy=0.72, loss=8.68, step=3e+4]  \nValid: 100% 5664/5667 [00:03<00:00, 1827.36 uttr/s, accuracy=0.69, loss=10.47]\n                                      \nTrain:   0% 0/2000 [00:00<?, ? step/s]                                                \nTrain:   0% 5/2000 [00:00<01:23, 23.99 step/s, accuracy=0.69, loss=10.51, step=3e+4]1]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Step 30000, best model saved. (accuracy=0.6879)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [01:10<00:00, 28.26 step/s, accuracy=0.69, loss=9.09, step=32000] \nValid: 100% 5664/5667 [00:03<00:00, 1818.34 uttr/s, accuracy=0.70, loss=10.36]\nTrain: 100% 2000/2000 [01:10<00:00, 28.30 step/s, accuracy=0.81, loss=9.30, step=34000] \nValid: 100% 5664/5667 [00:03<00:00, 1821.40 uttr/s, accuracy=0.71, loss=10.15]\nTrain: 100% 2000/2000 [01:09<00:00, 28.66 step/s, accuracy=0.84, loss=9.34, step=36000] \nValid: 100% 5664/5667 [00:03<00:00, 1806.92 uttr/s, accuracy=0.71, loss=10.14]\nTrain: 100% 2000/2000 [01:10<00:00, 28.47 step/s, accuracy=0.84, loss=8.50, step=38000] \nValid: 100% 5664/5667 [00:03<00:00, 1833.16 uttr/s, accuracy=0.71, loss=9.97] \nTrain: 100% 2000/2000 [01:10<00:00, 28.19 step/s, accuracy=0.66, loss=10.53, step=4e+4] \nValid: 100% 5664/5667 [00:03<00:00, 1791.79 uttr/s, accuracy=0.72, loss=9.90]\n                                      \nTrain:   0% 0/2000 [00:00<?, ? step/s]                                                \nTrain:   0% 5/2000 [00:00<01:20, 24.69 step/s, accuracy=0.94, loss=7.27, step=4e+4] 1]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Step 40000, best model saved. (accuracy=0.7156)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [01:10<00:00, 28.41 step/s, accuracy=0.88, loss=8.22, step=42000] \nValid: 100% 5664/5667 [00:03<00:00, 1806.05 uttr/s, accuracy=0.73, loss=9.79]\nTrain: 100% 2000/2000 [01:10<00:00, 28.50 step/s, accuracy=0.81, loss=8.14, step=44000] \nValid: 100% 5664/5667 [00:03<00:00, 1812.75 uttr/s, accuracy=0.73, loss=9.75]\nTrain: 100% 2000/2000 [01:10<00:00, 28.37 step/s, accuracy=0.78, loss=8.57, step=46000] \nValid: 100% 5664/5667 [00:03<00:00, 1746.94 uttr/s, accuracy=0.73, loss=9.65]\nTrain: 100% 2000/2000 [01:11<00:00, 28.02 step/s, accuracy=0.81, loss=8.63, step=48000] \nValid: 100% 5664/5667 [00:03<00:00, 1814.19 uttr/s, accuracy=0.74, loss=9.58]\nTrain: 100% 2000/2000 [01:10<00:00, 28.29 step/s, accuracy=0.78, loss=9.41, step=5e+4]  \nValid: 100% 5664/5667 [00:03<00:00, 1804.94 uttr/s, accuracy=0.74, loss=9.52]\n                                      \nTrain:   0% 0/2000 [00:00<?, ? step/s]                                                \nTrain:   0% 6/2000 [00:00<01:16, 26.11 step/s, accuracy=0.91, loss=7.11, step=5e+4]71]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Step 50000, best model saved. (accuracy=0.7401)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [01:11<00:00, 27.84 step/s, accuracy=0.84, loss=7.99, step=52000] \nValid: 100% 5664/5667 [00:03<00:00, 1856.93 uttr/s, accuracy=0.74, loss=9.47]\nTrain: 100% 2000/2000 [01:09<00:00, 28.78 step/s, accuracy=0.78, loss=8.39, step=54000] \nValid: 100% 5664/5667 [00:03<00:00, 1835.24 uttr/s, accuracy=0.74, loss=9.40]\nTrain: 100% 2000/2000 [01:10<00:00, 28.45 step/s, accuracy=0.91, loss=7.11, step=56000] \nValid: 100% 5664/5667 [00:03<00:00, 1833.95 uttr/s, accuracy=0.75, loss=9.37]\nTrain: 100% 2000/2000 [01:09<00:00, 28.70 step/s, accuracy=0.75, loss=8.40, step=58000] \nValid: 100% 5664/5667 [00:03<00:00, 1710.12 uttr/s, accuracy=0.75, loss=9.30]\nTrain: 100% 2000/2000 [01:08<00:00, 29.06 step/s, accuracy=0.78, loss=9.60, step=6e+4]  \nValid: 100% 5664/5667 [00:03<00:00, 1840.49 uttr/s, accuracy=0.75, loss=9.30]\n                                      \nTrain:   0% 0/2000 [00:00<?, ? step/s]                                                \nTrain:   0% 5/2000 [00:00<01:17, 25.80 step/s, accuracy=0.78, loss=8.70, step=6e+4]71]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Step 60000, best model saved. (accuracy=0.7525)\n","output_type":"stream"},{"name":"stderr","text":"Train: 100% 2000/2000 [01:09<00:00, 28.96 step/s, accuracy=0.84, loss=7.55, step=62000] \nValid: 100% 5664/5667 [00:03<00:00, 1826.15 uttr/s, accuracy=0.75, loss=9.33]\nTrain: 100% 2000/2000 [01:09<00:00, 28.58 step/s, accuracy=0.88, loss=7.34, step=64000] \nValid: 100% 5664/5667 [00:03<00:00, 1802.31 uttr/s, accuracy=0.75, loss=9.28]\nTrain: 100% 2000/2000 [01:10<00:00, 28.42 step/s, accuracy=0.84, loss=8.34, step=66000] \nValid: 100% 5664/5667 [00:03<00:00, 1826.79 uttr/s, accuracy=0.75, loss=9.26]\nTrain: 100% 2000/2000 [01:09<00:00, 28.66 step/s, accuracy=0.88, loss=7.36, step=68000] \nValid: 100% 5664/5667 [00:03<00:00, 1828.43 uttr/s, accuracy=0.75, loss=9.17]\nTrain: 100% 2000/2000 [01:10<00:00, 28.54 step/s, accuracy=0.78, loss=8.20, step=7e+4]  \nValid: 100% 5664/5667 [00:03<00:00, 1794.07 uttr/s, accuracy=0.75, loss=9.24]\n                                      \nTrain:   0% 0/2000 [00:00<?, ? step/s]                                                \nTrain:   0% 0/2000 [00:00<?, ? step/s]53 step/s, accuracy=0.72, loss=9.73, step=34971]\u001b[A\n","output_type":"stream"},{"name":"stdout","text":"Step 70000, best model saved. (accuracy=0.7535)\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"import os\nimport json\nimport torch\nfrom pathlib import Path\nfrom torch.utils.data import Dataset\n\n\nclass InferenceDataset(Dataset):\n    def __init__(self, data_dir):\n        testdata_path = Path(data_dir) / \"testdata.json\"\n        metadata = json.load(testdata_path.open())\n        self.data_dir = data_dir\n        self.data = metadata[\"utterances\"]\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        utterance = self.data[index]\n        feat_path = utterance[\"feature_path\"]\n        mel = torch.load(os.path.join(self.data_dir, feat_path))\n\n        return feat_path, mel\n\n\ndef inference_collate_batch(batch):\n    \"\"\"Collate a batch of data.\"\"\"\n    feat_paths, mels = zip(*batch)\n\n    return feat_paths, torch.stack(mels)","metadata":{"execution":{"iopub.status.busy":"2025-07-20T18:35:58.560081Z","iopub.execute_input":"2025-07-20T18:35:58.561076Z","iopub.status.idle":"2025-07-20T18:35:58.568783Z","shell.execute_reply.started":"2025-07-20T18:35:58.561039Z","shell.execute_reply":"2025-07-20T18:35:58.567682Z"},"trusted":true},"outputs":[],"execution_count":49},{"cell_type":"code","source":"import json\nimport csv\nfrom pathlib import Path\nfrom tqdm.notebook import tqdm\n\nimport torch\nfrom torch.utils.data import DataLoader\n\ndef parse_args():\n    \"\"\"arguments\"\"\"\n    config = {\n        \"data_dir\": \"/kaggle/input/ml2023springhw4/Dataset\",\n        \"model_path\": \"./model.ckpt\",\n        \"output_path\": \"./output.csv\",\n    }\n\n    return config\n\n\ndef main(\n    data_dir,\n    model_path,\n    output_path,\n):\n    \"\"\"Main function.\"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"[Info]: Use {device} now!\")\n\n    mapping_path = Path(data_dir) / \"mapping.json\"\n    mapping = json.load(mapping_path.open())\n\n    dataset = InferenceDataset(data_dir)\n    dataloader = DataLoader(\n        dataset,\n        batch_size=1,\n        shuffle=False,\n        drop_last=False,\n        num_workers=8,\n        collate_fn=inference_collate_batch,\n    )\n    print(f\"[Info]: Finish loading data!\",flush = True)\n\n    speaker_num = len(mapping[\"id2speaker\"])\n    model = Classifier(n_spks=speaker_num).to(device)\n    model.load_state_dict(torch.load(model_path))\n    model.eval()\n    print(f\"[Info]: Finish creating model!\",flush = True)\n\n    results = [[\"Id\", \"Category\"]]\n    for feat_paths, mels in tqdm(dataloader):\n        with torch.no_grad():\n            mels = mels.to(device)\n            outs = model(mels)\n            outs = model.pred_layer(outs)  # AMSoftmax\n            preds = outs.argmax(1).cpu().numpy()\n            for feat_path, pred in zip(feat_paths, preds):\n                results.append([feat_path, mapping[\"id2speaker\"][str(pred)]])\n\n    with open(output_path, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerows(results)\n\n\nif __name__ == \"__main__\":\n    main(**parse_args())","metadata":{"execution":{"iopub.status.busy":"2025-07-20T18:37:16.810023Z","iopub.execute_input":"2025-07-20T18:37:16.810400Z"},"trusted":true},"outputs":[{"name":"stdout","text":"[Info]: Use cuda now!\n[Info]: Finish loading data!\n[Info]: Finish creating model!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/8000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7653acda81564a2a8e415feb8cf472b7"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  if __name__ == \"__main__\":\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}